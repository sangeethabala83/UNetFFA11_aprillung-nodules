import os
import shutil
import random
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import backend as K
from skimage.metrics import structural_similarity as ssim
from tensorflow.keras.metrics import Mean

# METRICS DEFINITIONS
def precision_m(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    tp = K.sum(K.round(y_true * y_pred))
    fp = K.sum(K.round((1 - y_true) * y_pred))
    return tp / (tp + fp + K.epsilon())

def recall_m(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    tp = K.sum(K.round(y_true * y_pred))
    fn = K.sum(K.round(y_true * (1 - y_pred)))
    return tp / (tp + fn + K.epsilon())

def dice_coefficient(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + 1e-7) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1e-7)

def iou_m(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    intersection = K.sum(y_true * y_pred)
    union = K.sum(y_true) + K.sum(y_pred) - intersection
    return intersection / (union + K.epsilon())

def f1_score(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2 * (precision * recall) / (precision + recall + K.epsilon())

# 1. DATASET SPLITTING
def split_dataset(image_dir, mask_dir, output_base, train_ratio=0.8, val_ratio=0.1):
    os.makedirs(output_base, exist_ok=True)
    splits = ['train', 'val', 'test']
    for split in splits:
        os.makedirs(os.path.join(output_base, split, 'images'), exist_ok=True)
        os.makedirs(os.path.join(output_base, split, 'masks'), exist_ok=True)

    image_files = sorted(os.listdir(image_dir))
    mask_files = sorted(os.listdir(mask_dir)) # Get mask files as well

    print(f"Image files found: {len(image_files)}")
    print(f"Mask files found: {len(mask_files)}")

    print("Image files sample:", image_files[:10]) # Print a sample of image files
    print("Mask files sample:", mask_files[:10]) # Print a sample of mask files

    # Find valid image-mask pairs based on filenames
    valid_files = []
    for image_file in image_files:
        mask_file = "mask_" + image_file
        if mask_file in mask_files:
            valid_files.append(image_file)


    print(f"Total valid image-mask pairs: {len(valid_files)}")
    if not valid_files:
        print("No matching image and mask files found. Please check directories and filenames.")
        if image_files:
            print(f"First 5 image files: {image_files[:5]}")
        if mask_files:
            print(f"First 5 mask files: {mask_files[:5]}")
        return # Exit the function if no valid files are found


    random.shuffle(valid_files)

    total = len(valid_files)
    train_end = int(train_ratio * total)
    val_end = int((train_ratio + val_ratio) * total)

    split_files = {
        'train': valid_files[:train_end],
        'val': valid_files[train_end:val_end],
        'test': valid_files[val_end:]
    }

    for split in splits:
        for file in split_files[split]:
            shutil.copy(os.path.join(image_dir, file), os.path.join(output_base, split, 'images', file))
            shutil.copy(os.path.join(mask_dir, "mask_" + file), os.path.join(output_base, split, 'masks', "mask_" + file))

# 2. FRACTAL-FRACTIONAL ATTENTION COMPONENTS
class FractalFractionalIntegralMask:
    def __init__(self, alpha=2.0, beta=0.5, kernel_size=3):
        self.alpha = alpha
        self.beta = beta
        self.kernel_size = kernel_size

    def generate_mask(self):
        center = self.kernel_size // 2
        mask = np.zeros((self.kernel_size, self.kernel_size), dtype=np.float32)
        for i in range(self.kernel_size):
            for j in range(self.kernel_size):
                distance = np.sqrt((i - center) ** 2 + (j - center) ** 2)
                mask[i, j] = (1 + distance ** self.alpha) ** (-self.beta)
        mask /= np.sum(mask)
        return mask

    def apply_mask(self, feature_map):
        mask = self.generate_mask()
        mask_tensor = tf.convert_to_tensor(mask, dtype=tf.float32)
        mask_tensor = tf.expand_dims(tf.expand_dims(mask_tensor, -1), -1)
        return tf.nn.depthwise_conv2d(feature_map, mask_tensor, strides=[1, 1, 1, 1], padding='SAME')

def attention_block_with_fractal(g, x, filters):
    g_conv = tf.keras.layers.Conv2D(filters, (1, 1), activation='relu', padding='same')(g)
    x_conv = tf.keras.layers.Conv2D(filters, (1, 1), activation='relu', padding='same')(x)
    combined = tf.keras.layers.add([g_conv, x_conv])
    combined = tf.keras.layers.Activation('relu')(combined)
    combined = tf.keras.layers.Lambda(lambda z: tf.pow(z, 1.2))(combined)
    attention = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same')(combined)
    return tf.keras.layers.multiply([x, attention])

def nested_skip(input_tensor, filters):
    conv = tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(input_tensor)
    conv = tf.keras.layers.Conv2D(filters, (3, 3), activation='relu', padding='same')(conv)
    return conv

# 3. UNET WITH FRACTAL ATTENTION
def unet_plus_plus_with_fractal_attention_in_encoder(input_size=(256, 256, 1)):
    inputs = tf.keras.layers.Input(input_size)

    conv1 = nested_skip(inputs, 64)
    conv1 = attention_block_with_fractal(conv1, conv1, 64)
    pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = nested_skip(pool1, 128)
    conv2 = attention_block_with_fractal(conv2, conv2, 128)
    pool2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)

    conv3 = nested_skip(pool2, 256)
    conv3 = attention_block_with_fractal(conv3, conv3, 256)
    pool3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)

    conv4 = nested_skip(pool3, 512)

    up5 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv4)
    merge5 = tf.keras.layers.concatenate([up5, conv3], axis=3)
    merge5 = attention_block_with_fractal(merge5, conv3, 256)
    conv5 = nested_skip(merge5, 256)

    up6 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv5)
    merge6 = tf.keras.layers.concatenate([up6, conv2], axis=3)
    merge6 = attention_block_with_fractal(merge6, conv2, 128)
    conv6 = nested_skip(merge6, 128)

    up7 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv6)
    merge7 = tf.keras.layers.concatenate([up7, conv1], axis=3)
    merge7 = attention_block_with_fractal(merge7, conv1, 64)
    conv7 = nested_skip(merge7, 64)

    output = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(conv7)

    return tf.keras.Model(inputs=inputs, outputs=output)

# 4. LOAD IMAGES AND MASKS AS DATASET
def load_image_mask(image_path, mask_path, target_size=(256, 256), augment=True):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_png(img, channels=1)
    img = tf.image.resize(img, target_size)
    img = tf.cast(img, tf.float32) / 255.0

    mask = tf.io.read_file(mask_path)
    mask = tf.image.decode_png(mask, channels=1)
    mask = tf.image.resize(mask, target_size)
    mask = tf.cast(mask, tf.float32) / 255.0

    if augment:
        if tf.random.uniform(()) > 0.5:
            img = tf.image.flip_left_right(img)
            mask = tf.image.flip_left_right(mask)
        if tf.random.uniform(()) > 0.5:
            img = tf.image.flip_up_down(img)
            mask = tf.image.flip_up_down(mask)
        k = tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32)
        img = tf.image.rot90(img, k)
        mask = tf.image.rot90(mask, k)
        img = tf.image.random_brightness(img, max_delta=0.1)

    return img, mask


def get_dataset(image_folder, mask_folder, batch_size=8, shuffle=True):
    image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if os.path.exists(os.path.join(mask_folder, "mask_" + f))])
    mask_paths = [os.path.join(mask_folder, "mask_" + os.path.basename(f)) for f in image_paths]

    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))
    # Inspect the data types before mapping
    for element in dataset.take(1):
        print(f"Image path type: {element[0].dtype}, Mask path type: {element[1].dtype}")

    dataset = dataset.map(lambda x, y: load_image_mask(x, y), num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        dataset = dataset.shuffle(buffer_size=100)
    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# 5. MAIN SCRIPT
if __name__ == "__main__":
    original_image_dir = "/content/unzip/LIDC555/555/IMAGES"
    original_mask_dir = "/content/unzip/LIDC555/555/otsu_masks"
    output_split_dir = "/content/split_data"

    split_dataset(original_image_dir, original_mask_dir, output_split_dir)

    train_dataset = get_dataset(os.path.join(output_split_dir, 'train/images'),
                                os.path.join(output_split_dir, 'train/masks'))
    val_dataset = get_dataset(os.path.join(output_split_dir, 'val/images'),
                              os.path.join(output_split_dir, 'val/masks'))
    test_dataset = get_dataset(os.path.join(output_split_dir, 'test/images'),
                               os.path.join(output_split_dir, 'test/masks'),
                               batch_size=1, shuffle=False)

    model = unet_plus_plus_with_fractal_attention_in_encoder(input_size=(256, 256, 1))
    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    from tensorflow. keras.optimizers import Adam
    optimizer = Adam(learning_rate=1e-5)
    from tensorflow.keras.callbacks import ReduceLROnPlateau

    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)

    history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, verbose=1)
    model.save("unet_fractal_attention_1000.h5")

    results = model.evaluate(test_dataset, verbose=0)
    print("\nFinal Test Metrics:")
    for name, value in zip(model.metrics_names, results):
        print(f"{name}: {value:.4f}")

    # Calculate SSIM for 10 test images
    print("\nSSIM Scores for 10 Test Samples:")
    ssim_scores = []
    for i, (test_image, test_mask) in enumerate(test_dataset.take(10)):
        prediction = model.predict(test_image, verbose=0)
        test_mask_np = test_mask[0, :, :, 0].numpy()
        pred_mask_np = prediction[0, :, :, 0]
        ssim_value = ssim(test_mask_np, pred_mask_np, data_range=1.0)
        ssim_scores.append(ssim_value)
        print(f"Image {i+1} SSIM: {ssim_value:.4f}")

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 3, 1)
        plt.imshow(test_image[0, :, :, 0], cmap='gray')
        plt.title("Test Image")
        plt.axis('off')

        plt.subplot(1, 3, 2)
        plt.imshow(test_mask_np, cmap='gray')
        plt.title("True Mask")
        plt.axis('off')

        plt.subplot(1, 3, 3)
        plt.imshow(pred_mask_np, cmap='gray')
        plt.title("Predicted Mask")
        plt.axis('off')

        plt.show()

    print(f"\nAverage SSIM for 10 images: {np.mean(ssim_scores):.4f}")

    # Plot Training Curves
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Loss Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Accuracy Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.tight_layout()
    plt.show()

def evaluate_full_metrics(model, dataset):
    precision_avg = Mean()
    recall_avg = Mean()
    dice_avg = Mean()
    iou_avg = Mean()
    f1_avg = Mean()

    for x, y_true in dataset:
        y_pred = tf.cast(model(x, training=False) > 0.5, tf.float32)

        precision_avg.update_state(precision_m(y_true, y_pred))
        recall_avg.update_state(recall_m(y_true, y_pred))
        dice_avg.update_state(dice_coefficient(y_true, y_pred))
        iou_avg.update_state(iou_m(y_true, y_pred))
        f1_avg.update_state(f1_score(y_true, y_pred))

    print("\nEvaluation Metrics on Test Set:")
    print(f"Precision       : {precision_avg.result().numpy():.4f}")
    print(f"Recall          : {recall_avg.result().numpy():.4f}")
    print(f"Dice Coefficient: {dice_avg.result().numpy():.4f}")
    print(f"IoU             : {iou_avg.result().numpy():.4f}")
    print(f"F1 Score        : {f1_avg.result().numpy():.4f}")


if __name__ == "__main__":
    original_image_dir = "/content/unzip/LIDC555/555/IMAGES"
    original_mask_dir = "/content/unzip/LIDC555/555/otsu_masks"
    output_split_dir = "/content/split_data"

    split_dataset(original_image_dir, original_mask_dir, output_split_dir)

    train_dataset = get_dataset(os.path.join(output_split_dir, 'train/images'),
                                os.path.join(output_split_dir, 'train/masks'))
    val_dataset = get_dataset(os.path.join(output_split_dir, 'val/images'),
                              os.path.join(output_split_dir, 'val/masks'))
    test_dataset = get_dataset(os.path.join(output_split_dir, 'test/images'),
                               os.path.join(output_split_dir, 'test/masks'),
                               batch_size=1, shuffle=False)

    model = unet_plus_plus_with_fractal_attention_in_encoder(input_size=(256, 256, 1))
    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    from tensorflow. keras.optimizers import Adam
    optimizer = Adam(learning_rate=1e-5)
    from tensorflow.keras.callbacks import ReduceLROnPlateau

    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)

    history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, verbose=1)
    model.save("unet_fractal_attention_1000.h5")

    results = model.evaluate(test_dataset, verbose=0)
    print("\nFinal Test Metrics:")
    for name, value in zip(model.metrics_names, results):
        print(f"{name}: {value:.4f}")

    # Calculate SSIM for 10 test images
    print("\nSSIM Scores for 10 Test Samples:")
    ssim_scores = []
    for i, (test_image, test_mask) in enumerate(test_dataset.take(10)):
        prediction = model.predict(test_image, verbose=0)
        test_mask_np = test_mask[0, :, :, 0].numpy()
        pred_mask_np = prediction[0, :, :, 0]
        ssim_value = ssim(test_mask_np, pred_mask_np, data_range=1.0)
        ssim_scores.append(ssim_value)
        print(f"Image {i+1} SSIM: {ssim_value:.4f}")

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 3, 1)
        plt.imshow(test_image[0, :, :, 0], cmap='gray')
        plt.title("Test Image")
        plt.axis('off')

        plt.subplot(1, 3, 2)
        plt.imshow(test_mask_np, cmap='gray')
        plt.title("True Mask")
        plt.axis('off')

        plt.subplot(1, 3, 3)
        plt.imshow(pred_mask_np, cmap='gray')
        plt.title("Predicted Mask")
        plt.axis('off')

        plt.show()

    print(f"\nAverage SSIM for 10 images: {np.mean(ssim_scores):.4f}")

    # Plot Training Curves
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Loss Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Accuracy Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.tight_layout()
    plt.show()

    evaluate_full_metrics(model, test_dataset)

    # This part was causing errors and is redundant with the evaluation in evaluate_full_metrics
    # plt.imshow(pred_mask_np, cmap='gray')
    # for image, mask in test_dataset.take(1):
    #      pred = model(image, training=False)
    #      pred = tf.cast(pred > 0.5, tf.float32)
    #      plt.subplot(1,3,1); plt.imshow(image[0,...,0], cmap='gray'); plt.title("Image")
    #      plt.subplot(1,3,2); plt.imshow(mask[0,...,0], cmap='gray'); plt.title("True Mask")
    #      plt.subplot(1,3,3); plt.imshow(pred[0,...,0], cmap='gray'); plt.title("Pred Mask")
    #      plt.show()
    #      print("\nEvaluation Metrics on Test Set:")
    #      print(f"Precision       : {precision_avg.result().numpy():.4f}")
    #      print(f"Recall          : {recall_avg.result().numpy():.4f}")
    #      print(f"Dice Coefficient: {dice_avg.result().numpy():.4f}")
    #      print(f"IoU             : {iou_avg.result().numpy():.4f}")
    #      print(f"F1 Score        : {f1_avg.result().numpy():.4f}")

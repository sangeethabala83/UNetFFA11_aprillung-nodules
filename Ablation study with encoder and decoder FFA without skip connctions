def unet_ffa_encoder_decoder_no_skips(input_size=(256, 256, 1)):
    inputs = tf.keras.Input(shape=input_size)

    # Encoder with Fractal-Fractional Attention
    e1 = conv_block(inputs, 64)
    att1 = attention_block_with_fractal(e1, 64)
    p1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(att1)

    e2 = conv_block(p1, 128)
    att2 = attention_block_with_fractal(e2, 128)
    p2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(att2)

    e3 = conv_block(p2, 256)
    att3 = attention_block_with_fractal(e3, 256)
    p3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(att3)

    e4 = conv_block(p3, 512)
    att4 = attention_block_with_fractal(e4, 512)

    # Decoder with Fractal-Fractional Attention, No Skip Connections
    u5 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(att4)
    c5 = conv_block(u5, 256)
    att_dec5 = attention_block_with_fractal(c5, 256)

    u6 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(att_dec5)
    c6 = conv_block(u6, 128)
    att_dec6 = attention_block_with_fractal(c6, 128)

    u7 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(att_dec6)
    c7 = conv_block(u7, 64)
    att_dec7 = attention_block_with_fractal(c7, 64)

    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(att_dec7)

    return tf.keras.Model(inputs=inputs, outputs=outputs)

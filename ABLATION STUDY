# UNet-FFA decoder only
import os
import shutil
import random
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import backend as K
from skimage.metrics import structural_similarity as ssim
from tensorflow.keras.metrics import Mean
import math



# METRICS DEFINITIONS
def precision_m(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    tp = K.sum(K.round(y_true * y_pred))
    fp = K.sum(K.round((1 - y_true) * y_pred))
    return tp / (tp + fp + K.epsilon())

def recall_m(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    tp = K.sum(K.round(y_true * y_pred))
    fn = K.sum(K.round(y_true * (1 - y_pred)))
    return tp / (tp + fn + K.epsilon())

def dice_coefficient(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + 1e-7) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1e-7)

def iou_m(y_true, y_pred):
    y_pred = tf.cast(y_pred > 0.5, tf.float32)
    intersection = K.sum(y_true * y_pred)
    union = K.sum(y_true) + K.sum(y_pred) - intersection
    return intersection / (union + K.epsilon())

def f1_score(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2 * (precision * recall) / (precision + recall + K.epsilon())

# 1. DATASET SPLITTING
def split_dataset(image_dir, mask_dir, output_base, train_ratio=0.8, val_ratio=0.1):
    os.makedirs(output_base, exist_ok=True)
    splits = ['train', 'val', 'test']
    for split in splits:
        os.makedirs(os.path.join(output_base, split, 'images'), exist_ok=True)
        os.makedirs(os.path.join(output_base, split, 'masks'), exist_ok=True)

    image_files = sorted(os.listdir(image_dir))
    mask_files = sorted(os.listdir(mask_dir)) # Get mask files as well

    print(f"Image files found: {len(image_files)}")
    print(f"Mask files found: {len(mask_files)}")

    print("Image files sample:", image_files[:10]) # Print a sample of image files
    print("Mask files sample:", mask_files[:10]) # Print a sample of mask files

    # Find valid image-mask pairs based on filenames
    valid_files = []
    for image_file in image_files:
        mask_file = "mask_" + image_file
        if mask_file in mask_files:
            valid_files.append(image_file)


    print(f"Total valid image-mask pairs: {len(valid_files)}")
    if not valid_files:
        print("No matching image and mask files found. Please check directories and filenames.")
        if image_files:
            print(f"First 5 image files: {image_files[:5]}")
        if mask_files:
            print(f"First 5 mask files: {mask_files[:5]}")
        return # Exit the function if no valid files are found


    random.shuffle(valid_files)

    total = len(valid_files)
    train_end = int(train_ratio * total)
    val_end = int((train_ratio + val_ratio) * total)

    split_files = {
        'train': valid_files[:train_end],
        'val': valid_files[train_end:val_end],
        'test': valid_files[val_end:]
    }

    for split in splits:
        for file in split_files[split]:
            shutil.copy(os.path.join(image_dir, file), os.path.join(output_base, split, 'images', file))
            shutil.copy(os.path.join(mask_dir, "mask_" + file), os.path.join(output_base, split, 'masks', "mask_" + file))

# 2. FRACTAL-FRACTIONAL ATTENTION COMPONENTS
import tensorflow as tf
import numpy as np

# FRACTAL-FRACTIONAL ATTENTION MASK
class FractalFractionalIntegralMask:
    def __init__(self, alpha=2.0, beta=0.5, kernel_size=3):
        self.alpha = alpha
        self.beta = beta
        self.kernel_size = kernel_size

    def generate_mask(self):
        center = self.kernel_size // 2
        mask = np.zeros((self.kernel_size, self.kernel_size), dtype=np.float32)
        for i in range(self.kernel_size):
            for j in range(self.kernel_size):
                distance = np.sqrt((i - center) ** 2 + (j - center) ** 2)
                mask[i, j] = (1 + distance ** self.alpha) ** (-self.beta)
        mask /= np.sum(mask)
        return mask

    def apply_mask(self, feature_map):
        mask = self.generate_mask()
        mask_tensor = tf.convert_to_tensor(mask, dtype=tf.float32)
        mask_tensor = tf.expand_dims(tf.expand_dims(mask_tensor, -1), -1)
        return tf.nn.depthwise_conv2d(feature_map, mask_tensor, strides=[1, 1, 1, 1], padding='SAME')

# ATTENTION BLOCK WITH FRACTAL
def attention_block_with_fractal(g, filters):
    g_conv = tf.keras.layers.Conv2D(filters, (1, 1), activation='relu', padding='same')(g)
    x_conv = tf.keras.layers.Conv2D(filters, (1, 1), activation='relu', padding='same')(g)
    combined = tf.keras.layers.add([g_conv, x_conv])
    combined = tf.keras.layers.Activation('relu')(combined)
    combined = tf.keras.layers.Lambda(lambda z: tf.pow(z, 1.2))(combined)
    attention = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same')(combined)
    return tf.keras.layers.multiply([g, attention])

# BASIC CONV BLOCK
def conv_block(x, filters):
    x = tf.keras.layers.Conv2D(filters, 3, activation='relu', padding='same')(x)
    x = tf.keras.layers.Conv2D(filters, 3, activation='relu', padding='same')(x)
    return x

# UNET WITH FFA IN DECODER ONLY (WITH SKIP CONNECTIONS)
def unet_ffa_decoder_with_skips(input_size=(256, 256, 1)):
    inputs = tf.keras.Input(shape=input_size)

    # Encoder
    e1 = conv_block(inputs, 64)
    p1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e1)

    e2 = conv_block(p1, 128)
    p2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e2)

    e3 = conv_block(p2, 256)
    p3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(e3)

    e4 = conv_block(p3, 512)

    # Decoder with FFA and Skip Connections
    u5 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(e4)
    concat5 = tf.keras.layers.concatenate([u5, e3], axis=3)
    att5 = attention_block_with_fractal(concat5, 256)
    c5 = conv_block(att5, 256)

    u6 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(c5)
    concat6 = tf.keras.layers.concatenate([u6, e2], axis=3)
    att6 = attention_block_with_fractal(concat6, 128)
    c6 = conv_block(att6, 128)

    u7 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(c6)
    concat7 = tf.keras.layers.concatenate([u7, e1], axis=3)
    att7 = attention_block_with_fractal(concat7, 64)
    c7 = conv_block(att7, 64)

    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(c7)

    return tf.keras.Model(inputs=inputs, outputs=outputs)


# 4. LOAD IMAGES AND MASKS AS DATASET
def load_image_mask(image_path, mask_path, target_size=(256, 256), augment=True):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_png(img, channels=1)
    img = tf.image.resize(img, target_size)
    img = tf.cast(img, tf.float32) / 255.0

    mask = tf.io.read_file(mask_path)
    mask = tf.image.decode_png(mask, channels=1)
    mask = tf.image.resize(mask, target_size)
    mask = tf.cast(mask, tf.float32) / 255.0

    if augment:
        if tf.random.uniform(()) > 0.5:
            img = tf.image.flip_left_right(img)
            mask = tf.image.flip_left_right(mask)
        if tf.random.uniform(()) > 0.5:
            img = tf.image.flip_up_down(img)
            mask = tf.image.flip_up_down(mask)
        k = tf.random.uniform([], minval=0, maxval=4, dtype=tf.int32)
        img = tf.image.rot90(img, k)
        mask = tf.image.rot90(mask, k)
        img = tf.image.random_brightness(img, max_delta=0.1)

    return img, mask


def get_dataset(image_folder, mask_folder, batch_size=8, shuffle=True):
    image_paths = sorted([os.path.join(image_folder, f) for f in os.listdir(image_folder) if os.path.exists(os.path.join(mask_folder, "mask_" + f))])
    mask_paths = [os.path.join(mask_folder, "mask_" + os.path.basename(f)) for f in image_paths]

    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))
    # Inspect the data types before mapping
    for element in dataset.take(1):
        print(f"Image path type: {element[0].dtype}, Mask path type: {element[1].dtype}")

    dataset = dataset.map(lambda x, y: load_image_mask(x, y), num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        dataset = dataset.shuffle(buffer_size=100)
    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# 5. MAIN SCRIPT
if __name__ == "__main__":
    original_image_dir = "/content/unzip/Miracle100/miracle100/IMAGES100LIDC"
    original_mask_dir = "/content/unzip/Miracle100/miracle100/otsu_masks"
    output_split_dir = "/content/split_data"

    split_dataset(original_image_dir, original_mask_dir, output_split_dir)

    train_dataset = get_dataset(os.path.join(output_split_dir, 'train/images'),
                                os.path.join(output_split_dir, 'train/masks'))
    val_dataset = get_dataset(os.path.join(output_split_dir, 'val/images'),
                              os.path.join(output_split_dir, 'val/masks'))
    test_dataset = get_dataset(os.path.join(output_split_dir, 'test/images'),
                               os.path.join(output_split_dir, 'test/masks'),
                               batch_size=1, shuffle=False)

    model = unet_ffa_decoder_with_skips(input_size=(256, 256, 1))


    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    from tensorflow. keras.optimizers import Adam
    optimizer = Adam(learning_rate=1e-5)
    from tensorflow.keras.callbacks import ReduceLROnPlateau

    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)

    history = model.fit(train_dataset, validation_data=val_dataset, epochs=20, verbose=1)
    model.save("unet_fractal_attention_1000.h5")

    results = model.evaluate(test_dataset, verbose=0)
    print("\nFinal Test Metrics:")
    for name, value in zip(model.metrics_names, results):
        print(f"{name}: {value:.4f}")

    # Calculate SSIM for 10 test images
    print("\nSSIM Scores for 10 Test Samples:")
    ssim_scores = []
    for i, (test_image, test_mask) in enumerate(test_dataset.take(10)):
        prediction = model.predict(test_image, verbose=0)
        test_mask_np = test_mask[0, :, :, 0].numpy()
        pred_mask_np = prediction[0, :, :, 0]
        ssim_value = ssim(test_mask_np, pred_mask_np, data_range=1.0)
        ssim_scores.append(ssim_value)
        print(f"Image {i+1} SSIM: {ssim_value:.4f}")

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 3, 1)
        plt.imshow(test_image[0, :, :, 0], cmap='gray')
        plt.title("Test Image")
        plt.axis('off')

        plt.subplot(1, 3, 2)
        plt.imshow(test_mask_np, cmap='gray')
        plt.title("True Mask")
        plt.axis('off')

        plt.subplot(1, 3, 3)
        plt.imshow(pred_mask_np, cmap='gray')
        plt.title("Predicted Mask")
        plt.axis('off')

        plt.show()

    print(f"\nAverage SSIM for 10 images: {np.mean(ssim_scores):.4f}")

    # Plot Training Curves
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Val Loss')
    plt.title('Loss Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Val Accuracy')
    plt.title('Accuracy Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score

def evaluate_full_metrics(model, dataset):
    precision_list = []
    recall_list = []
    f1_list = []
    dice_list = []
    iou_list = []

    for x_batch, y_true_batch in dataset:
        y_pred_batch = model.predict(x_batch, verbose=0)
        y_pred_batch = (y_pred_batch > 0.5).astype(np.uint8)
        y_true_batch = y_true_batch.numpy().astype(np.uint8)

        for i in range(len(y_true_batch)):
            y_true = y_true_batch[i, :, :, 0].flatten()
            y_pred = y_pred_batch[i, :, :, 0].flatten()

            if np.sum(y_true) == 0 and np.sum(y_pred) == 0:
                # Skip empty masks to avoid undefined metrics
                continue

            precision_list.append(precision_score(y_true, y_pred, zero_division=0))
            recall_list.append(recall_score(y_true, y_pred, zero_division=0))
            f1_list.append(f1_score(y_true, y_pred, zero_division=0))

            intersection = np.logical_and(y_true, y_pred).sum()
            union = np.logical_or(y_true, y_pred).sum()
            iou = intersection / (union + 1e-7)
            dice = 2 * intersection / (np.sum(y_true) + np.sum(y_pred) + 1e-7)

            iou_list.append(iou)
            dice_list.append(dice)

    print("\nEvaluation Metrics on Test Set:")
    print(f"Precision       : {np.mean(precision_list):.4f}")
    print(f"Recall          : {np.mean(recall_list):.4f}")
    print(f"F1 Score        : {np.mean(f1_list):.4f}")
    print(f"Dice Coefficient: {np.mean(dice_list):.4f}")
    print(f"IoU             : {np.mean(iou_list):.4f}")
evaluate_full_metrics(model, test_dataset)
